# üìò Cours : Introduction et mise en place de Ceph

## 1. Pr√©sentation de Ceph

Ceph est une **solution de stockage distribu√© open source** con√ßue pour offrir :
- **Haute disponibilit√©**
- **Tol√©rance aux pannes**
- **√âvolutivit√© horizontale** (on peut ajouter des n≈ìuds sans interruption)

Il permet de fournir trois types de stockage :
1. **Ceph Block Storage (RBD)** : disques virtuels pour les VM (OpenNebula, OpenStack, etc.)
2. **Ceph Object Storage (RGW)** : compatible S3 / Swift
3. **Ceph File System (CephFS)** : syst√®me de fichiers distribu√©

---

## 2. Architecture et composants Ceph

Un cluster Ceph est constitu√© de plusieurs **daemons** et **r√¥les** :

### üß± 2.1. MON (Monitor)
- Maintient la **topologie du cluster** (qui est en ligne, qui est mort).
- Stocke la **carte du cluster** (cluster map).
- N√©cessaire : **3 MON minimum** pour une haute disponibilit√© (quorum).

### üíæ 2.2. OSD (Object Storage Daemon)
- C‚Äôest le c≈ìur du stockage.
- Chaque OSD g√®re **un disque**.
- Les donn√©es sont **r√©pliqu√©es ou dispers√©es** entre plusieurs OSD pour assurer la r√©silience.
- Exemple : 3 OSD ‚áí 3 disques ‚áí r√©plication 3√ó.

### üß† 2.3. MGR (Manager)
- Fournit des informations de **statistiques** et **tableaux de bord Web (ceph-mgr-dashboard)**.
- N√©cessaire √† partir de Ceph Luminous.

### üìÅ 2.4. MDS (Metadata Server)
- Utilis√© uniquement pour **CephFS**.
- G√®re les m√©tadonn√©es du syst√®me de fichiers (permissions, arborescence, etc.).

### üåê 2.5. RGW (RADOS Gateway)
- Fournit une **API de stockage objet** compatible avec S3 / Swift.

---

## 3. Principe de fonctionnement

1. Les donn√©es sont stock√©es dans un **pool** (ensemble logique).
2. Le pool est divis√© en **Placement Groups (PG)**.
3. Les PG sont distribu√©s entre les **OSD** selon une **carte CRUSH**.
4. Le client (VM, serveur, etc.) √©crit directement sur les OSD via les librairies Ceph ‚Äî pas besoin de passer par un serveur central !

---

## 4. Sch√©ma simplifi√©

```
+-----------------------+
|        Client         |
| (VM / OpenNebula RBD) |
+----------+------------+
           |
           v
+-------------------------------+
|         Ceph Cluster          |
| +------+  +------+  +------+  |
| | MON  |  | MGR  |  | MDS  |  |
| +------+  +------+  +------+  |
| +--------+ +--------+ +--------+
| |  OSD1  | |  OSD2  | |  OSD3  |
| +--------+ +--------+ +--------+
+-------------------------------+
```

---

## 5. Installation de Ceph

### üîß 5.1. Pr√©-requis
- Au moins **3 serveurs** pour un vrai cluster.
- OS : **Rocky Linux 9 / Ubuntu 22.04**.
- SSH root sans mot de passe (par cl√© ssh) entre les n≈ìuds.
- Chaque disque d√©di√© au Ceph doit √™tre **vide** (pas de partition).

### üî© 5.2. Installation via Cephadm

#### √âtape 1 : Installation du bootstrap
```bash
dnf install -y cephadm
cephadm bootstrap --mon-ip <IP_MON_PRINCIPAL> --initial-dashboard-user admin --initial-dashboard-password admin
```

#### √âtape 2 : Ajouter les h√¥tes
```bash
ceph orch host add <nom_hote> <ip_hote>
ceph orch host label add <nom_hote> osd
```

#### √âtape 3 : Ajouter les OSD
```bash
ceph orch daemon add osd <nom_hote>:<chemin_du_disque>
# Exemple :
ceph orch daemon add osd node1:/dev/sdb
```

#### √âtape 4 : V√©rifier l‚Äô√©tat du cluster
```bash
ceph -s
```

---

## 6. Commandes de base

| Commande | Description |
|-----------|-------------|
| `ceph -s` | √âtat g√©n√©ral du cluster |
| `ceph osd tree` | Affiche la hi√©rarchie des OSD |
| `ceph osd df` | Espace disque par OSD |
| `ceph health` | Sant√© du cluster |
| `ceph mon stat` | Statut des moniteurs |
| `ceph df` | Espace total utilis√© / disponible |
| `rbd ls` | Liste des volumes RBD |
| `rbd create <nom> --size <taille> --pool <pool>` | Cr√©e un disque RBD |
| `rbd info <nom>` | D√©tails sur un disque RBD |

---

## 7. Bonnes pratiques

- Minimum **3 MON + 3 OSD** pour la redondance.
- Utiliser des **disques d√©di√©s** pour les OSD (pas de partage avec le syst√®me).
- Configurer un **r√©seau s√©par√© pour Ceph (backend)**.
- Surveiller r√©guli√®rement via le **Ceph Dashboard** ou Prometheus/Grafana.

---

## 8. Int√©gration avec OpenNebula

OpenNebula peut utiliser Ceph comme **datastore principal** :
- Les **images, templates et snapshots** sont stock√©s dans un **pool RBD**.
- Les **VM KVM** utilisent ces volumes directement comme disques.
- En cas de migration, OpenNebula n‚Äôa pas besoin de copier le disque ‚Äî il est d√©j√† partag√© via Ceph.

Exemple de datastore Ceph dans OpenNebula :
```bash
one datastore create ceph_system --type SYSTEM --ds_mad ceph --tm_mad ceph --safe_dirs /var/tmp --pool ceph-pool
```

---

## 9. Conclusion

Ceph est une brique **cl√©** pour tout environnement cloud distribu√© (OpenNebula, OpenStack, Kubernetes).
Il assure un **stockage partag√©, r√©silient et √©volutif**, id√©al pour des infrastructures p√©dagogiques ou de production.

---

‚úçÔ∏è **Prochain TP : D√©ploiement complet d‚Äôun cluster Ceph 3 n≈ìuds avec OpenNebula + KVM**
